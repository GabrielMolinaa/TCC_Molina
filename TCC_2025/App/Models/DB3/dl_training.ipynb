{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc25e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados: (71611, 500, 12)\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"D:\\Stash\\Datasets\\db2_janelado\\db2_5g_all_data.npz\"\n",
    "\n",
    "data = np.load(data_path)\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "print(\"Formato final dos dados:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9bbf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_mpp_mzp(X):\n",
    "    \"\"\"\n",
    "    Calcula MPP e MZP para cada amostra no dataset X.\n",
    "    Entrada:\n",
    "        X: np.array de shape (samples, time_steps, channels)\n",
    "    Retorno:\n",
    "        np.array de shape (samples, 2) ‚Üí MPP e MZP por amostra\n",
    "    \"\"\"\n",
    "    mpp = []\n",
    "    mzp = []\n",
    "\n",
    "    for sample in X:\n",
    "        # ¬µ0: energia do sinal (por canal)\n",
    "        mu0 = np.sqrt(np.sum(sample ** 2, axis=0))  # shape: (channels,)\n",
    "\n",
    "        # ¬µ2: energia da 1¬™ derivada (por canal)\n",
    "        delta = np.diff(sample, axis=0)\n",
    "        mu2 = np.sqrt(np.sum(delta ** 2, axis=0))  # shape: (channels,)\n",
    "\n",
    "        # ¬µ4: energia da 2¬™ derivada (por canal)\n",
    "        delta2 = np.diff(delta, axis=0)\n",
    "        mu4 = np.sqrt(np.sum(delta2 ** 2, axis=0))  # shape: (channels,)\n",
    "\n",
    "        # Œ® = ¬µ4 / ¬µ2, Œ¶ = ¬µ2 / ¬µ0\n",
    "        psi = np.nan_to_num(mu4 / mu2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        phi = np.nan_to_num(mu2 / mu0, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # MPP = ¬µ0 * Œ®, MZP = ¬µ0 * Œ¶\n",
    "        mpp_sample = mu0 * psi\n",
    "        mzp_sample = mu0 * phi\n",
    "\n",
    "        # Concatena os dois vetores em um s√≥ (por amostra)\n",
    "        mpp_mzp_concat = np.concatenate([mpp_sample, mzp_sample])  # shape: (channels * 2,)\n",
    "        mpp.append(mpp_sample)\n",
    "        mzp.append(mzp_sample)\n",
    "\n",
    "    # Concatena tudo (samples, features)\n",
    "    return np.stack(mpp), np.stack(mzp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d173b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 19s]\n",
      "val_accuracy: 0.7192241549491882\n",
      "\n",
      "Best val_accuracy So Far: 0.7345296144485474\n",
      "Total elapsed time: 00h 08m 50s\n",
      "\u001b[1m  1/237\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m25s\u001b[0m 108ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 38 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m237/237\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Melhor acur√°cia CNN otimizada (DB2 - 5 gestos): 0.7345\n",
      "Melhores hiperpar√¢metros encontrados:\n",
      "{'conv1_filters': 32, 'conv1_kernel': 5, 'conv2_filters': 256, 'conv2_kernel': 5, 'conv3_filters': 128, 'conv3_kernel': 3, 'dense1_units': 128, 'l2': 0.004739667552102512, 'dropout1': 0.4, 'dense2_units': 192, 'dropout2': 0.4, 'lr': 0.0001985090560549791}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Activation, BatchNormalization, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "db2_5g = r\"D:\\Stash\\Datasets\\db2_janelado\\db2_5g_all_data.npz\"\n",
    "db2_10g = r\"D:\\Stash\\Datasets\\db2_janelado\\db2_10g_all_data.npz\"\n",
    "\n",
    "db3_5g = r\"D:\\Stash\\Datasets\\db3_janelado\\db3_5g_all_data.npz\"\n",
    "db3_10g = r\"D:\\Stash\\Datasets\\db3_janelado\\db3_10g_all_data.npz\"\n",
    "\n",
    "db5_5g = r\"D:\\Stash\\Datasets\\db5_janelado\\db5_5g_all_data.npz\"\n",
    "db5_10g = r\"D:\\Stash\\Datasets\\db5_janelado\\db5_10g_all_data.npz\"\n",
    "\n",
    "\n",
    "data = np.load(data_path)\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "print(\"Formato final dos dados:\", X_train.shape)\n",
    "\n",
    "mpp_train, mzp_train = calculate_mpp_mzp(X_train)\n",
    "mpp_test, mzp_test = calculate_mpp_mzp(X_test)\n",
    "\n",
    "X_train_mpp_mzp = np.concatenate([mpp_train, mzp_train], axis=-1)  # shape: (samples, features)\n",
    "X_test_mpp_mzp = np.concatenate([mpp_test, mzp_test], axis=-1)\n",
    "\n",
    "X_train_final = X_train_mpp_mzp[..., np.newaxis]  # shape: (samples, features, 1)\n",
    "X_test_final = X_test_mpp_mzp[..., np.newaxis]\n",
    "# X_train_final = X_train  # shape: (samples, features, 1)\n",
    "# X_test_final = X_test\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_final.shape[1], 1)),\n",
    "        \n",
    "        \n",
    "        Conv1D(filters=hp.Choice('conv1_filters', [16, 32, 64, 128]),\n",
    "               kernel_size=hp.Choice('conv1_kernel', [2, 3, 5, 7]),\n",
    "               padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=hp.Choice('conv2_filters', [32, 64, 128, 256]),\n",
    "               kernel_size=hp.Choice('conv2_kernel', [2, 3, 5]),\n",
    "               padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=hp.Choice('conv3_filters', [16, 32, 64, 128]),\n",
    "               kernel_size=hp.Choice('conv3_kernel', [2, 3, 5]),\n",
    "               padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        \n",
    "        GlobalAveragePooling1D(),\n",
    "        \n",
    "    \n",
    "        Dense(units=hp.Int('dense1_units', 64, 512, step=64),\n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "        Dropout(hp.Float('dropout1', 0.3, 0.7, step=0.1)),\n",
    "        \n",
    "        Dense(units=hp.Int('dense2_units', 32, 256, step=32),\n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "        Dropout(hp.Float('dropout2', 0.3, 0.7, step=0.1)),\n",
    "        \n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=hp.Float('lr', 1e-5, 1e-3, sampling='log')),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    directory='tuner_dir_db5_5g_new',\n",
    "    project_name='cnn_gesture_classification'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "tuner.search(X_train_final, y_train,\n",
    "             epochs=30,\n",
    "             batch_size=128,\n",
    "             validation_data=(X_test_final, y_test),\n",
    "             callbacks=[early_stop],\n",
    "             verbose=2)\n",
    "\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "\n",
    "y_pred = np.argmax(best_model.predict(X_test_final), axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Melhor acur√°cia CNN otimizada (DB2 - 5 gestos): {accuracy:.4f}\")\n",
    "print(\"Melhores hiperpar√¢metros encontrados:\")\n",
    "print(best_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ccc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 01m 22s]\n",
      "val_accuracy: 0.6334514617919922\n",
      "\n",
      "Best val_accuracy So Far: 0.651043713092804\n",
      "Total elapsed time: 01h 13m 22s\n",
      "\u001b[1m  1/477\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:06\u001b[0m 140ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 38 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m477/477\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "‚úÖ Melhor acur√°cia para DB5_10G: 0.6510\n",
      "Melhores hiperpar√¢metros para DB5_10G: {'conv1_filters': 64, 'conv1_kernel': 7, 'conv2_filters': 256, 'conv2_kernel': 3, 'conv3_filters': 128, 'conv3_kernel': 5, 'dense1_units': 384, 'l2': 0.0017738646500971463, 'dropout1': 0.4, 'dense2_units': 224, 'dropout2': 0.3, 'lr': 0.001}\n",
      "‚úÖ Resultados salvos em CNN_resultados_todosDBs.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Activation, BatchNormalization, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_mpp_mzp(X):\n",
    "    mpp, mzp = [], []\n",
    "    for sample in X:\n",
    "        mu0 = np.sqrt(np.sum(sample ** 2, axis=0))\n",
    "        delta = np.diff(sample, axis=0)\n",
    "        mu2 = np.sqrt(np.sum(delta ** 2, axis=0))\n",
    "        delta2 = np.diff(delta, axis=0)\n",
    "        mu4 = np.sqrt(np.sum(delta2 ** 2, axis=0))\n",
    "        psi = np.nan_to_num(mu4 / mu2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        phi = np.nan_to_num(mu2 / mu0, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mpp.append(mu0 * psi)\n",
    "        mzp.append(mu0 * phi)\n",
    "    return np.stack(mpp), np.stack(mzp)\n",
    "\n",
    "datasets = {\n",
    "    \"DB2_5G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_5g_all_data.npz\",\n",
    "    \"DB2_10G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_10g_all_data.npz\",\n",
    "    \"DB3_5G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_5g_all_data.npz\",\n",
    "    \"DB3_10G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_10g_all_data.npz\",\n",
    "    \"DB5_5G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_5g_all_data.npz\",\n",
    "    \"DB5_10G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_10g_all_data.npz\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for db_name, data_path in datasets.items():\n",
    "    print(f\"\\nüîπ Iniciando treinamento para {db_name}\")\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    mpp_train, mzp_train = calculate_mpp_mzp(X_train)\n",
    "    mpp_test, mzp_test = calculate_mpp_mzp(X_test)\n",
    "    X_train_final = np.concatenate([mpp_train, mzp_train], axis=-1)[..., np.newaxis]\n",
    "    X_test_final = np.concatenate([mpp_test, mzp_test], axis=-1)[..., np.newaxis]\n",
    "\n",
    "    def build_model(hp):\n",
    "        model = Sequential([\n",
    "            Input(shape=(X_train_final.shape[1], 1)),\n",
    "            Conv1D(filters=hp.Choice('conv1_filters', [16, 32, 64, 128]), kernel_size=hp.Choice('conv1_kernel', [2, 3, 5, 7]), padding='same', kernel_initializer='he_normal'),\n",
    "            BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "            Conv1D(filters=hp.Choice('conv2_filters', [32, 64, 128, 256]), kernel_size=hp.Choice('conv2_kernel', [2, 3, 5]), padding='same', kernel_initializer='he_normal'),\n",
    "            BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "            Conv1D(filters=hp.Choice('conv3_filters', [16, 32, 64, 128]), kernel_size=hp.Choice('conv3_kernel', [2, 3, 5]), padding='same', kernel_initializer='he_normal'),\n",
    "            BatchNormalization(), Activation('relu'),\n",
    "            GlobalAveragePooling1D(),\n",
    "            Dense(units=hp.Int('dense1_units', 64, 512, step=64), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "            Dropout(hp.Float('dropout1', 0.3, 0.7, step=0.1)),\n",
    "            Dense(units=hp.Int('dense2_units', 32, 256, step=32), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "            Dropout(hp.Float('dropout2', 0.3, 0.7, step=0.1)),\n",
    "            Dense(len(np.unique(y_train)), activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=hp.Float('lr', 1e-5, 1e-3, sampling='log')), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    project_dir = os.path.join(\"CNN_tuner_dir\", db_name)\n",
    "    tuner = kt.BayesianOptimization(build_model, objective='val_accuracy', max_trials=50, directory=project_dir, project_name='cnn_gesture_classification')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history_callback = History()\n",
    "    \n",
    "    tuner.search(X_train_final, y_train, epochs=30, batch_size=128, validation_data=(X_test_final, y_test), callbacks=[early_stop, history_callback], verbose=1)\n",
    "\n",
    "    best_model = tuner.get_best_models(1)[0]\n",
    "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "    y_pred = np.argmax(best_model.predict(X_test_final), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"‚úÖ Melhor acur√°cia para {db_name}: {accuracy:.4f}\")\n",
    "    print(f\"Melhores hiperpar√¢metros para {db_name}: {best_hp.values}\")\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        'Dataset': db_name,\n",
    "        'Accuracy': accuracy,\n",
    "        **best_hp.values\n",
    "    })\n",
    "\n",
    "    if history_callback.history:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(history_callback.history['val_loss'], label='Validation Loss')\n",
    "        plt.plot(history_callback.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'Curvas de Valida√ß√£o para {db_name}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"curva_{db_name}.png\")\n",
    "        plt.close()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"CNN_resultados_todosDBs.csv\", index=False)\n",
    "print(\"‚úÖ Resultados salvos em CNN_resultados_todosDBs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273265e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Treinando e salvando modelo para: DB2_5G\n",
      "Epoch 1/50\n",
      "560/560 - 12s - 22ms/step - accuracy: 0.4623 - loss: 1.5848 - val_accuracy: 0.4752 - val_loss: 1.4036\n",
      "Epoch 2/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.6177 - loss: 1.0493 - val_accuracy: 0.6247 - val_loss: 1.0407\n",
      "Epoch 3/50\n",
      "560/560 - 12s - 21ms/step - accuracy: 0.6742 - loss: 0.9319 - val_accuracy: 0.6600 - val_loss: 0.9379\n",
      "Epoch 4/50\n",
      "560/560 - 11s - 20ms/step - accuracy: 0.7142 - loss: 0.8513 - val_accuracy: 0.6834 - val_loss: 0.9669\n",
      "Epoch 5/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.7419 - loss: 0.7880 - val_accuracy: 0.6582 - val_loss: 1.0288\n",
      "Epoch 6/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.7582 - loss: 0.7526 - val_accuracy: 0.7279 - val_loss: 0.7929\n",
      "Epoch 7/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.7793 - loss: 0.6967 - val_accuracy: 0.6548 - val_loss: 1.0421\n",
      "Epoch 8/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.7877 - loss: 0.6787 - val_accuracy: 0.7752 - val_loss: 0.7130\n",
      "Epoch 9/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8036 - loss: 0.6436 - val_accuracy: 0.7449 - val_loss: 0.7588\n",
      "Epoch 10/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8123 - loss: 0.6198 - val_accuracy: 0.7658 - val_loss: 0.7337\n",
      "Epoch 11/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8203 - loss: 0.6002 - val_accuracy: 0.7197 - val_loss: 0.8727\n",
      "Epoch 12/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8273 - loss: 0.5801 - val_accuracy: 0.7837 - val_loss: 0.7082\n",
      "Epoch 13/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8374 - loss: 0.5553 - val_accuracy: 0.6185 - val_loss: 1.4539\n",
      "Epoch 14/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8383 - loss: 0.5503 - val_accuracy: 0.7451 - val_loss: 0.8020\n",
      "Epoch 15/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8478 - loss: 0.5280 - val_accuracy: 0.7766 - val_loss: 0.7275\n",
      "Epoch 16/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8510 - loss: 0.5135 - val_accuracy: 0.7412 - val_loss: 0.8650\n",
      "Epoch 17/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8605 - loss: 0.4963 - val_accuracy: 0.7908 - val_loss: 0.7245\n",
      "Epoch 18/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8626 - loss: 0.4884 - val_accuracy: 0.6631 - val_loss: 1.1623\n",
      "Epoch 19/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8658 - loss: 0.4764 - val_accuracy: 0.7180 - val_loss: 0.9576\n",
      "Epoch 20/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8692 - loss: 0.4693 - val_accuracy: 0.7694 - val_loss: 0.7848\n",
      "Epoch 21/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8753 - loss: 0.4530 - val_accuracy: 0.7696 - val_loss: 0.8094\n",
      "Epoch 22/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8756 - loss: 0.4451 - val_accuracy: 0.7894 - val_loss: 0.7039\n",
      "Epoch 23/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8812 - loss: 0.4342 - val_accuracy: 0.8023 - val_loss: 0.6950\n",
      "Epoch 24/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8803 - loss: 0.4363 - val_accuracy: 0.7945 - val_loss: 0.7655\n",
      "Epoch 25/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8822 - loss: 0.4307 - val_accuracy: 0.7939 - val_loss: 0.7096\n",
      "Epoch 26/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8877 - loss: 0.4093 - val_accuracy: 0.7677 - val_loss: 0.8061\n",
      "Epoch 27/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8902 - loss: 0.4069 - val_accuracy: 0.7777 - val_loss: 0.8314\n",
      "Epoch 28/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8911 - loss: 0.4055 - val_accuracy: 0.7615 - val_loss: 0.8445\n",
      "Epoch 29/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8925 - loss: 0.3990 - val_accuracy: 0.7428 - val_loss: 0.9686\n",
      "Epoch 30/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8946 - loss: 0.3909 - val_accuracy: 0.8082 - val_loss: 0.6652\n",
      "Epoch 31/50\n",
      "560/560 - 10s - 19ms/step - accuracy: 0.8965 - loss: 0.3824 - val_accuracy: 0.8091 - val_loss: 0.6912\n",
      "Epoch 32/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8976 - loss: 0.3816 - val_accuracy: 0.7934 - val_loss: 0.7618\n",
      "Epoch 33/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.8990 - loss: 0.3783 - val_accuracy: 0.8074 - val_loss: 0.7104\n",
      "Epoch 34/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.9027 - loss: 0.3683 - val_accuracy: 0.8003 - val_loss: 0.7414\n",
      "Epoch 35/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.9019 - loss: 0.3629 - val_accuracy: 0.8105 - val_loss: 0.6890\n",
      "Epoch 36/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.9045 - loss: 0.3570 - val_accuracy: 0.8078 - val_loss: 0.6927\n",
      "Epoch 37/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.9082 - loss: 0.3488 - val_accuracy: 0.8017 - val_loss: 0.7194\n",
      "Epoch 38/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.9103 - loss: 0.3433 - val_accuracy: 0.8070 - val_loss: 0.6863\n",
      "Epoch 39/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.9085 - loss: 0.3486 - val_accuracy: 0.7975 - val_loss: 0.8498\n",
      "Epoch 40/50\n",
      "560/560 - 10s - 18ms/step - accuracy: 0.9096 - loss: 0.3442 - val_accuracy: 0.7878 - val_loss: 0.8444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo salvo como: modelo_CNN_otimizado_DB2_5G.h5\n",
      "\n",
      "üîß Treinando e salvando modelo para: DB2_10G\n",
      "Epoch 1/50\n",
      "1095/1095 - 22s - 20ms/step - accuracy: 0.4722 - loss: 1.7752 - val_accuracy: 0.4747 - val_loss: 1.7270\n",
      "Epoch 2/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.6419 - loss: 1.1521 - val_accuracy: 0.6494 - val_loss: 1.1352\n",
      "Epoch 3/50\n",
      "1095/1095 - 20s - 19ms/step - accuracy: 0.7107 - loss: 0.9640 - val_accuracy: 0.6317 - val_loss: 1.2267\n",
      "Epoch 4/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.7488 - loss: 0.8503 - val_accuracy: 0.6944 - val_loss: 1.0296\n",
      "Epoch 5/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.7801 - loss: 0.7678 - val_accuracy: 0.6951 - val_loss: 0.9860\n",
      "Epoch 6/50\n",
      "1095/1095 - 19s - 18ms/step - accuracy: 0.7999 - loss: 0.7085 - val_accuracy: 0.7206 - val_loss: 0.9492\n",
      "Epoch 7/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8161 - loss: 0.6602 - val_accuracy: 0.7255 - val_loss: 0.9543\n",
      "Epoch 8/50\n",
      "1095/1095 - 19s - 18ms/step - accuracy: 0.8319 - loss: 0.6161 - val_accuracy: 0.7014 - val_loss: 1.0364\n",
      "Epoch 9/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8420 - loss: 0.5850 - val_accuracy: 0.7129 - val_loss: 1.0053\n",
      "Epoch 10/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8525 - loss: 0.5532 - val_accuracy: 0.7235 - val_loss: 0.9718\n",
      "Epoch 11/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8595 - loss: 0.5336 - val_accuracy: 0.7408 - val_loss: 0.9630\n",
      "Epoch 12/50\n",
      "1095/1095 - 19s - 18ms/step - accuracy: 0.8668 - loss: 0.5102 - val_accuracy: 0.7089 - val_loss: 1.0606\n",
      "Epoch 13/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8729 - loss: 0.4915 - val_accuracy: 0.7413 - val_loss: 0.9359\n",
      "Epoch 14/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8789 - loss: 0.4748 - val_accuracy: 0.6686 - val_loss: 1.3038\n",
      "Epoch 15/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8854 - loss: 0.4557 - val_accuracy: 0.7280 - val_loss: 1.0440\n",
      "Epoch 16/50\n",
      "1095/1095 - 19s - 18ms/step - accuracy: 0.8898 - loss: 0.4413 - val_accuracy: 0.7566 - val_loss: 0.9268\n",
      "Epoch 17/50\n",
      "1095/1095 - 20s - 18ms/step - accuracy: 0.8934 - loss: 0.4301 - val_accuracy: 0.7556 - val_loss: 0.9074\n",
      "Epoch 18/50\n",
      "1095/1095 - 19s - 18ms/step - accuracy: 0.8997 - loss: 0.4121 - val_accuracy: 0.7704 - val_loss: 0.8658\n",
      "Epoch 19/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9011 - loss: 0.4073 - val_accuracy: 0.7295 - val_loss: 1.0438\n",
      "Epoch 20/50\n",
      "1095/1095 - 19s - 18ms/step - accuracy: 0.9060 - loss: 0.3930 - val_accuracy: 0.7201 - val_loss: 1.1350\n",
      "Epoch 21/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9079 - loss: 0.3855 - val_accuracy: 0.6821 - val_loss: 1.3192\n",
      "Epoch 22/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9105 - loss: 0.3767 - val_accuracy: 0.6926 - val_loss: 1.2038\n",
      "Epoch 23/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9120 - loss: 0.3743 - val_accuracy: 0.7536 - val_loss: 0.9653\n",
      "Epoch 24/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9158 - loss: 0.3591 - val_accuracy: 0.7621 - val_loss: 0.9036\n",
      "Epoch 25/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9174 - loss: 0.3556 - val_accuracy: 0.7517 - val_loss: 1.0024\n",
      "Epoch 26/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9195 - loss: 0.3453 - val_accuracy: 0.7525 - val_loss: 0.9828\n",
      "Epoch 27/50\n",
      "1095/1095 - 19s - 17ms/step - accuracy: 0.9226 - loss: 0.3382 - val_accuracy: 0.7702 - val_loss: 0.9158\n",
      "Epoch 28/50\n",
      "1095/1095 - 19s - 18ms/step - accuracy: 0.9246 - loss: 0.3324 - val_accuracy: 0.7366 - val_loss: 1.0346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo salvo como: modelo_CNN_otimizado_DB2_10G.h5\n",
      "\n",
      "üîß Treinando e salvando modelo para: DB3_5G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_28724\\3189033294.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  psi = np.nan_to_num(mu4 / mu2)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_28724\\3189033294.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  phi = np.nan_to_num(mu2 / mu0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "148/148 - 4s - 28ms/step - accuracy: 0.4104 - loss: 1.4403 - val_accuracy: 0.4289 - val_loss: 1.4139\n",
      "Epoch 2/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.6221 - loss: 1.0321 - val_accuracy: 0.5563 - val_loss: 1.1749\n",
      "Epoch 3/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.6868 - loss: 0.8755 - val_accuracy: 0.5825 - val_loss: 1.2013\n",
      "Epoch 4/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.7346 - loss: 0.7518 - val_accuracy: 0.5000 - val_loss: 1.5452\n",
      "Epoch 5/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.7536 - loss: 0.7050 - val_accuracy: 0.5977 - val_loss: 1.2006\n",
      "Epoch 6/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.7676 - loss: 0.6621 - val_accuracy: 0.6175 - val_loss: 1.1588\n",
      "Epoch 7/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.7836 - loss: 0.6212 - val_accuracy: 0.5740 - val_loss: 1.5361\n",
      "Epoch 8/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.7937 - loss: 0.5878 - val_accuracy: 0.6093 - val_loss: 1.2040\n",
      "Epoch 9/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8042 - loss: 0.5554 - val_accuracy: 0.5490 - val_loss: 1.7292\n",
      "Epoch 10/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8107 - loss: 0.5436 - val_accuracy: 0.5899 - val_loss: 1.5348\n",
      "Epoch 11/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8255 - loss: 0.5117 - val_accuracy: 0.5985 - val_loss: 1.4738\n",
      "Epoch 12/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8267 - loss: 0.5007 - val_accuracy: 0.6168 - val_loss: 1.3188\n",
      "Epoch 13/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8339 - loss: 0.4792 - val_accuracy: 0.5972 - val_loss: 1.4297\n",
      "Epoch 14/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8390 - loss: 0.4685 - val_accuracy: 0.6291 - val_loss: 1.4107\n",
      "Epoch 15/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8397 - loss: 0.4582 - val_accuracy: 0.6124 - val_loss: 1.4651\n",
      "Epoch 16/50\n",
      "148/148 - 2s - 13ms/step - accuracy: 0.8475 - loss: 0.4383 - val_accuracy: 0.6271 - val_loss: 1.2846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo salvo como: modelo_CNN_otimizado_DB3_5G.h5\n",
      "\n",
      "üîß Treinando e salvando modelo para: DB3_10G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_28724\\3189033294.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  psi = np.nan_to_num(mu4 / mu2)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_28724\\3189033294.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  phi = np.nan_to_num(mu2 / mu0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "304/304 - 6s - 19ms/step - accuracy: 0.1747 - loss: 2.5564 - val_accuracy: 0.2458 - val_loss: 2.4345\n",
      "Epoch 2/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.2580 - loss: 2.3398 - val_accuracy: 0.3017 - val_loss: 2.2138\n",
      "Epoch 3/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.3316 - loss: 2.1236 - val_accuracy: 0.3692 - val_loss: 2.0472\n",
      "Epoch 4/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.3926 - loss: 1.9539 - val_accuracy: 0.4054 - val_loss: 1.9464\n",
      "Epoch 5/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.4387 - loss: 1.8231 - val_accuracy: 0.4121 - val_loss: 1.8959\n",
      "Epoch 6/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.4723 - loss: 1.7233 - val_accuracy: 0.4394 - val_loss: 1.8285\n",
      "Epoch 7/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.4959 - loss: 1.6469 - val_accuracy: 0.4268 - val_loss: 1.8568\n",
      "Epoch 8/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.5174 - loss: 1.5801 - val_accuracy: 0.4515 - val_loss: 1.8017\n",
      "Epoch 9/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.5409 - loss: 1.5210 - val_accuracy: 0.4362 - val_loss: 1.8405\n",
      "Epoch 10/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.5504 - loss: 1.4760 - val_accuracy: 0.4722 - val_loss: 1.7499\n",
      "Epoch 11/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.5688 - loss: 1.4328 - val_accuracy: 0.4649 - val_loss: 1.7698\n",
      "Epoch 12/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.5842 - loss: 1.3890 - val_accuracy: 0.4670 - val_loss: 1.7887\n",
      "Epoch 13/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.5926 - loss: 1.3511 - val_accuracy: 0.4679 - val_loss: 1.7626\n",
      "Epoch 14/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6017 - loss: 1.3254 - val_accuracy: 0.4739 - val_loss: 1.7846\n",
      "Epoch 15/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6120 - loss: 1.2937 - val_accuracy: 0.4657 - val_loss: 1.7755\n",
      "Epoch 16/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6204 - loss: 1.2658 - val_accuracy: 0.4889 - val_loss: 1.7351\n",
      "Epoch 17/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6293 - loss: 1.2402 - val_accuracy: 0.4728 - val_loss: 1.7899\n",
      "Epoch 18/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6379 - loss: 1.2133 - val_accuracy: 0.4769 - val_loss: 1.7456\n",
      "Epoch 19/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6486 - loss: 1.1855 - val_accuracy: 0.4681 - val_loss: 1.8024\n",
      "Epoch 20/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6536 - loss: 1.1688 - val_accuracy: 0.4816 - val_loss: 1.7660\n",
      "Epoch 21/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6559 - loss: 1.1507 - val_accuracy: 0.4780 - val_loss: 1.8235\n",
      "Epoch 22/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6646 - loss: 1.1260 - val_accuracy: 0.4706 - val_loss: 1.8373\n",
      "Epoch 23/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6701 - loss: 1.1100 - val_accuracy: 0.4882 - val_loss: 1.7469\n",
      "Epoch 24/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6758 - loss: 1.0907 - val_accuracy: 0.4860 - val_loss: 1.7775\n",
      "Epoch 25/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6787 - loss: 1.0796 - val_accuracy: 0.4775 - val_loss: 1.8337\n",
      "Epoch 26/50\n",
      "304/304 - 3s - 10ms/step - accuracy: 0.6843 - loss: 1.0590 - val_accuracy: 0.4787 - val_loss: 1.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo salvo como: modelo_CNN_otimizado_DB3_10G.h5\n",
      "\n",
      "üîß Treinando e salvando modelo para: DB5_5G\n",
      "Epoch 1/50\n",
      "123/123 - 4s - 35ms/step - accuracy: 0.4324 - loss: 1.8732 - val_accuracy: 0.5779 - val_loss: 1.6270\n",
      "Epoch 2/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.6300 - loss: 1.4135 - val_accuracy: 0.6548 - val_loss: 1.2999\n",
      "Epoch 3/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.6953 - loss: 1.2178 - val_accuracy: 0.6688 - val_loss: 1.2197\n",
      "Epoch 4/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.7374 - loss: 1.0856 - val_accuracy: 0.6682 - val_loss: 1.2291\n",
      "Epoch 5/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.7634 - loss: 0.9899 - val_accuracy: 0.7110 - val_loss: 1.0858\n",
      "Epoch 6/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.7838 - loss: 0.9131 - val_accuracy: 0.7125 - val_loss: 1.0785\n",
      "Epoch 7/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8056 - loss: 0.8407 - val_accuracy: 0.6697 - val_loss: 1.1968\n",
      "Epoch 8/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8190 - loss: 0.7826 - val_accuracy: 0.7249 - val_loss: 1.0331\n",
      "Epoch 9/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8274 - loss: 0.7408 - val_accuracy: 0.7232 - val_loss: 1.0198\n",
      "Epoch 10/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8419 - loss: 0.6936 - val_accuracy: 0.7158 - val_loss: 1.0513\n",
      "Epoch 11/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8507 - loss: 0.6540 - val_accuracy: 0.7208 - val_loss: 1.0109\n",
      "Epoch 12/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8600 - loss: 0.6217 - val_accuracy: 0.7304 - val_loss: 0.9933\n",
      "Epoch 13/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8691 - loss: 0.5873 - val_accuracy: 0.7010 - val_loss: 1.0832\n",
      "Epoch 14/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.8783 - loss: 0.5546 - val_accuracy: 0.7022 - val_loss: 1.0840\n",
      "Epoch 15/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8813 - loss: 0.5297 - val_accuracy: 0.6893 - val_loss: 1.1418\n",
      "Epoch 16/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.8870 - loss: 0.5116 - val_accuracy: 0.7273 - val_loss: 1.0347\n",
      "Epoch 17/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.8947 - loss: 0.4789 - val_accuracy: 0.7289 - val_loss: 0.9970\n",
      "Epoch 18/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.9082 - loss: 0.4461 - val_accuracy: 0.7246 - val_loss: 1.0092\n",
      "Epoch 19/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.9081 - loss: 0.4308 - val_accuracy: 0.7291 - val_loss: 1.0010\n",
      "Epoch 20/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.9188 - loss: 0.4104 - val_accuracy: 0.7277 - val_loss: 1.0362\n",
      "Epoch 21/50\n",
      "123/123 - 2s - 17ms/step - accuracy: 0.9161 - loss: 0.3966 - val_accuracy: 0.7323 - val_loss: 1.0004\n",
      "Epoch 22/50\n",
      "123/123 - 2s - 18ms/step - accuracy: 0.9211 - loss: 0.3831 - val_accuracy: 0.7275 - val_loss: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo salvo como: modelo_CNN_otimizado_DB5_5G.h5\n",
      "\n",
      "üîß Treinando e salvando modelo para: DB5_10G\n",
      "Epoch 1/50\n",
      "241/241 - 6s - 26ms/step - accuracy: 0.5092 - loss: 1.8994 - val_accuracy: 0.5180 - val_loss: 1.7415\n",
      "Epoch 2/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.6785 - loss: 1.2202 - val_accuracy: 0.5528 - val_loss: 1.5433\n",
      "Epoch 3/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.7358 - loss: 0.9806 - val_accuracy: 0.5830 - val_loss: 1.4672\n",
      "Epoch 4/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.7747 - loss: 0.8326 - val_accuracy: 0.6376 - val_loss: 1.3097\n",
      "Epoch 5/50\n",
      "241/241 - 4s - 17ms/step - accuracy: 0.8013 - loss: 0.7298 - val_accuracy: 0.5887 - val_loss: 1.5475\n",
      "Epoch 6/50\n",
      "241/241 - 4s - 17ms/step - accuracy: 0.8277 - loss: 0.6527 - val_accuracy: 0.6364 - val_loss: 1.3118\n",
      "Epoch 7/50\n",
      "241/241 - 4s - 17ms/step - accuracy: 0.8501 - loss: 0.5851 - val_accuracy: 0.5828 - val_loss: 1.6125\n",
      "Epoch 8/50\n",
      "241/241 - 4s - 17ms/step - accuracy: 0.8676 - loss: 0.5289 - val_accuracy: 0.5963 - val_loss: 1.6197\n",
      "Epoch 9/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.8797 - loss: 0.4946 - val_accuracy: 0.6149 - val_loss: 1.6527\n",
      "Epoch 10/50\n",
      "241/241 - 4s - 17ms/step - accuracy: 0.8969 - loss: 0.4471 - val_accuracy: 0.4932 - val_loss: 2.4451\n",
      "Epoch 11/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.9048 - loss: 0.4175 - val_accuracy: 0.6519 - val_loss: 1.3829\n",
      "Epoch 12/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.9180 - loss: 0.3815 - val_accuracy: 0.6454 - val_loss: 1.4786\n",
      "Epoch 13/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.9269 - loss: 0.3510 - val_accuracy: 0.6426 - val_loss: 1.5004\n",
      "Epoch 14/50\n",
      "241/241 - 4s - 16ms/step - accuracy: 0.9336 - loss: 0.3320 - val_accuracy: 0.6392 - val_loss: 1.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo salvo como: modelo_CNN_otimizado_DB5_10G.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Activation, BatchNormalization, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "\n",
    "# === Fun√ß√£o para calcular MPP e MZP ===\n",
    "def calculate_mpp_mzp(X):\n",
    "    mpp, mzp = [], []\n",
    "    for sample in X:\n",
    "        mu0 = np.sqrt(np.sum(sample ** 2, axis=0))\n",
    "        delta = np.diff(sample, axis=0)\n",
    "        mu2 = np.sqrt(np.sum(delta ** 2, axis=0))\n",
    "        delta2 = np.diff(delta, axis=0)\n",
    "        mu4 = np.sqrt(np.sum(delta2 ** 2, axis=0))\n",
    "        psi = np.nan_to_num(mu4 / mu2)\n",
    "        phi = np.nan_to_num(mu2 / mu0)\n",
    "        mpp.append(mu0 * psi)\n",
    "        mzp.append(mu0 * phi)\n",
    "    return np.stack(mpp), np.stack(mzp)\n",
    "\n",
    "# === Datasets e hiperpar√¢metros otimizados ===\n",
    "config = [\n",
    "    (\"DB2_5G\", r\"D:\\Stash\\Datasets\\db2_janelado\\db2_5g_all_data.npz\", {\n",
    "        'conv1_filters':128,'conv1_kernel':2,'conv2_filters':256,'conv2_kernel':5,'conv3_filters':128,'conv3_kernel':5,\n",
    "        'dense1_units':448,'l2':0.01,'dropout1':0.3,'dense2_units':32,'dropout2':0.6,'lr':0.001\n",
    "    }),\n",
    "    (\"DB2_10G\", r\"D:\\Stash\\Datasets\\db2_janelado\\db2_10g_all_data.npz\", {\n",
    "        'conv1_filters':128,'conv1_kernel':7,'conv2_filters':256,'conv2_kernel':5,'conv3_filters':128,'conv3_kernel':5,\n",
    "        'dense1_units':256,'l2':0.0060122,'dropout1':0.5,'dense2_units':160,'dropout2':0.3,'lr':0.001\n",
    "    }),\n",
    "    (\"DB3_5G\", r\"D:\\Stash\\Datasets\\db3_janelado\\db3_5g_all_data.npz\", {\n",
    "        'conv1_filters':128,'conv1_kernel':7,'conv2_filters':256,'conv2_kernel':2,'conv3_filters':64,'conv3_kernel':3,\n",
    "        'dense1_units':320,'l2':0.0004108,'dropout1':0.6,'dense2_units':64,'dropout2':0.3,'lr':0.001\n",
    "    }),\n",
    "    (\"DB3_10G\", r\"D:\\Stash\\Datasets\\db3_janelado\\db3_10g_all_data.npz\", {\n",
    "        'conv1_filters':32,'conv1_kernel':3,'conv2_filters':128,'conv2_kernel':5,'conv3_filters':128,'conv3_kernel':5,\n",
    "        'dense1_units':512,'l2':0.0008011,'dropout1':0.5,'dense2_units':160,'dropout2':0.4,'lr':0.0001133\n",
    "    }),\n",
    "    (\"DB5_5G\", r\"D:\\Stash\\Datasets\\db5_janelado\\db5_5g_all_data.npz\", {\n",
    "        'conv1_filters':128,'conv1_kernel':5,'conv2_filters':256,'conv2_kernel':3,'conv3_filters':64,'conv3_kernel':5,\n",
    "        'dense1_units':320,'l2':0.001925,'dropout1':0.4,'dense2_units':128,'dropout2':0.5,'lr':0.000196\n",
    "    }),\n",
    "    (\"DB5_10G\", r\"D:\\Stash\\Datasets\\db5_janelado\\db5_10g_all_data.npz\", {\n",
    "        'conv1_filters':64,'conv1_kernel':7,'conv2_filters':256,'conv2_kernel':3,'conv3_filters':128,'conv3_kernel':5,\n",
    "        'dense1_units':384,'l2':0.001773,'dropout1':0.4,'dense2_units':224,'dropout2':0.3,'lr':0.001\n",
    "    })\n",
    "]\n",
    "\n",
    "# === Treinamento e salvamento ===\n",
    "for name, data_path, hp in config:\n",
    "    print(f\"\\nüîß Treinando e salvando modelo para: {name}\")\n",
    "\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "    X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    mpp_train, mzp_train = calculate_mpp_mzp(X_train)\n",
    "    mpp_test, mzp_test = calculate_mpp_mzp(X_test)\n",
    "\n",
    "    X_train_final = np.concatenate([mpp_train, mzp_train], axis=-1)[..., np.newaxis]\n",
    "    X_test_final = np.concatenate([mpp_test, mzp_test], axis=-1)[..., np.newaxis]\n",
    "\n",
    "    # Modelo com hiperpar√¢metros fixos\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_final.shape[1], 1)),\n",
    "        Conv1D(filters=hp['conv1_filters'], kernel_size=hp['conv1_kernel'], padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=hp['conv2_filters'], kernel_size=hp['conv2_kernel'], padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=hp['conv3_filters'], kernel_size=hp['conv3_kernel'], padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(units=hp['dense1_units'], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(hp['l2'])),\n",
    "        Dropout(hp['dropout1']),\n",
    "        Dense(units=hp['dense2_units'], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(hp['l2'])),\n",
    "        Dropout(hp['dropout2']),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp['lr']),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_final, y_train, epochs=50, batch_size=128, validation_data=(X_test_final, y_test), callbacks=[early_stop], verbose=2)\n",
    "\n",
    "\n",
    "    model_name = f\"modelo_CNN_otimizado_{name}.h5\"\n",
    "    model.save(model_name)\n",
    "    print(f\"‚úÖ Modelo salvo como: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Activation, BatchNormalization, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_mpp_mzp(X):\n",
    "    mpp, mzp = [], []\n",
    "    for sample in X:\n",
    "        mu0 = np.sqrt(np.sum(sample ** 2, axis=0))\n",
    "        delta = np.diff(sample, axis=0)\n",
    "        mu2 = np.sqrt(np.sum(delta ** 2, axis=0))\n",
    "        delta2 = np.diff(delta, axis=0)\n",
    "        mu4 = np.sqrt(np.sum(delta2 ** 2, axis=0))\n",
    "        psi = np.nan_to_num(mu4 / mu2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        phi = np.nan_to_num(mu2 / mu0, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mpp.append(mu0 * psi)\n",
    "        mzp.append(mu0 * phi)\n",
    "    return np.stack(mpp), np.stack(mzp)\n",
    "\n",
    "datasets = {\n",
    "    \"DB2_5G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_5g_all_data.npz\",\n",
    "    \"DB2_10G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_10g_all_data.npz\",\n",
    "    \"DB3_5G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_5g_all_data.npz\",\n",
    "    \"DB3_10G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_10g_all_data.npz\",\n",
    "    \"DB5_5G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_5g_all_data.npz\",\n",
    "    \"DB5_10G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_10g_all_data.npz\"\n",
    "}\n",
    "\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for db_name, data_path in datasets.items():\n",
    "    print(f\"\\nüî∏ Treinamento BASELINE para {db_name}\")\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    mpp_train, mzp_train = calculate_mpp_mzp(X_train)\n",
    "    mpp_test, mzp_test = calculate_mpp_mzp(X_test)\n",
    "    X_train_final = np.concatenate([mpp_train, mzp_train], axis=-1)[..., np.newaxis]\n",
    "    X_test_final = np.concatenate([mpp_test, mzp_test], axis=-1)[..., np.newaxis]\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_final.shape[1], 1)),\n",
    "        Conv1D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train_final, y_train, epochs=30, batch_size=128, validation_data=(X_test_final, y_test), callbacks=[early_stop], verbose=1)\n",
    "\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test_final), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"‚úÖ Acur√°cia BASELINE para {db_name}: {accuracy:.4f}\")\n",
    "\n",
    "    baseline_results.append({\n",
    "        'Dataset': db_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'conv1_filters': 64,\n",
    "        'conv1_kernel': 3,\n",
    "        'conv2_filters': 128,\n",
    "        'conv2_kernel': 3,\n",
    "        'conv3_filters': 64,\n",
    "        'conv3_kernel': 3,\n",
    "        'dense1_units': 256,\n",
    "        'dense2_units': 128,\n",
    "        'dropout1': 0.5,\n",
    "        'dropout2': 0.5,\n",
    "        'l2': 1e-3,\n",
    "        'lr': 1e-4\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Curvas BASELINE para {db_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"curva_baseline_{db_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "baseline_df.to_csv(\"CNN_baseline_todosDBs.csv\", index=False)\n",
    "print(\"‚úÖ Resultados BASELINE salvos em CNN_baseline_todosDBs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdf204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Activation, BatchNormalization, Input, LSTM, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_mpp_mzp(X):\n",
    "    mpp, mzp = [], []\n",
    "    for sample in X:\n",
    "        mu0 = np.sqrt(np.sum(sample ** 2, axis=0))\n",
    "        delta = np.diff(sample, axis=0)\n",
    "        mu2 = np.sqrt(np.sum(delta ** 2, axis=0))\n",
    "        delta2 = np.diff(delta, axis=0)\n",
    "        mu4 = np.sqrt(np.sum(delta2 ** 2, axis=0))\n",
    "        psi = np.nan_to_num(mu4 / mu2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        phi = np.nan_to_num(mu2 / mu0, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mpp.append(mu0 * psi)\n",
    "        mzp.append(mu0 * phi)\n",
    "    return np.stack(mpp), np.stack(mzp)\n",
    "\n",
    "datasets = {\n",
    "    \"DB2_5G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_5g_all_data.npz\",\n",
    "    \"DB2_10G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_10g_all_data.npz\",\n",
    "    \"DB3_5G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_5g_all_data.npz\",\n",
    "    \"DB3_10G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_10g_all_data.npz\",\n",
    "    \"DB5_5G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_5g_all_data.npz\",\n",
    "    \"DB5_10G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_10g_all_data.npz\"\n",
    "}\n",
    "\n",
    "hybrid_results = []\n",
    "\n",
    "for db_name, data_path in datasets.items():\n",
    "    print(f\"\\nüî∏ Treinamento H√çBRIDO CNN+LSTM para {db_name}\")\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    mpp_train, mzp_train = calculate_mpp_mzp(X_train)\n",
    "    mpp_test, mzp_test = calculate_mpp_mzp(X_test)\n",
    "    X_train_final = np.concatenate([mpp_train, mzp_train], axis=-1)[..., np.newaxis]\n",
    "    X_test_final = np.concatenate([mpp_test, mzp_test], axis=-1)[..., np.newaxis]\n",
    "\n",
    "    # Modelo H√çBRIDO CNN + LSTM\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_final.shape[1], 1)),\n",
    "        Conv1D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(), Activation('relu'),\n",
    "        Reshape((-1, 64)),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train_final, y_train, epochs=30, batch_size=128, validation_data=(X_test_final, y_test), callbacks=[early_stop], verbose=1)\n",
    "\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test_final), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"‚úÖ Acur√°cia H√çBRIDA para {db_name}: {accuracy:.4f}\")\n",
    "\n",
    "    hybrid_results.append({\n",
    "        'Dataset': db_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'conv1_filters': 64,\n",
    "        'conv1_kernel': 3,\n",
    "        'conv2_filters': 128,\n",
    "        'conv2_kernel': 3,\n",
    "        'conv3_filters': 64,\n",
    "        'conv3_kernel': 3,\n",
    "        'lstm_units': 64,\n",
    "        'dense1_units': 256,\n",
    "        'dense2_units': 128,\n",
    "        'dropout1': 0.5,\n",
    "        'dropout2': 0.5,\n",
    "        'l2': 1e-3,\n",
    "        'lr': 1e-4\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Curvas H√çBRIDAS para {db_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"curva_hibrido_{db_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "hybrid_df = pd.DataFrame(hybrid_results)\n",
    "hybrid_df.to_csv(\"CNN_LSTM_hibrido_todosDBs.csv\", index=False)\n",
    "print(\"‚úÖ Resultados H√çBRIDOS salvos em CNN_LSTM_hibrido_todosDBs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 02m 24s]\n",
      "val_accuracy: 0.6018773913383484\n",
      "\n",
      "Best val_accuracy So Far: 0.6521596312522888\n",
      "Total elapsed time: 01h 06m 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 44 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m477/477\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "‚úÖ Melhor acur√°cia H√çBRIDA (otimizada) para DB5_10G: 0.6522"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhores hiperpar√¢metros para DB5_10G: {'conv1_filters': 16, 'conv1_kernel': 7, 'conv2_filters': 128, 'conv2_kernel': 5, 'conv3_filters': 64, 'conv3_kernel': 5, 'reshape_features': 128, 'lstm_units': 128, 'dense1_units': 128, 'l2': 0.00015996109792129216, 'dropout1': 0.5, 'dense2_units': 32, 'dropout2': 0.3, 'lr': 0.0007765767620141046}\n",
      "‚úÖ Modelo salvo como modelo_hibrido_otimizado_DB5_10G.h5\n",
      "‚úÖ Resultados H√çBRIDOS OTIMIZADOS salvos em CNN_LSTM_hibrido_otimizado_todosDBs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_14728\\803235727.py:103: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Activation, BatchNormalization, Input, LSTM, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_mpp_mzp(X):\n",
    "    mpp, mzp = [], []\n",
    "    for sample in X:\n",
    "        mu0 = np.sqrt(np.sum(sample ** 2, axis=0))\n",
    "        delta = np.diff(sample, axis=0)\n",
    "        mu2 = np.sqrt(np.sum(delta ** 2, axis=0))\n",
    "        delta2 = np.diff(delta, axis=0)\n",
    "        mu4 = np.sqrt(np.sum(delta2 ** 2, axis=0))\n",
    "        psi = np.nan_to_num(mu4 / mu2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        phi = np.nan_to_num(mu2 / mu0, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mpp.append(mu0 * psi)\n",
    "        mzp.append(mu0 * phi)\n",
    "    return np.stack(mpp), np.stack(mzp)\n",
    "\n",
    "datasets = {\n",
    "    \"DB2_5G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_5g_all_data.npz\",\n",
    "    \"DB2_10G\": r\"D:\\Stash\\Datasets\\db2_janelado\\db2_10g_all_data.npz\",\n",
    "    \"DB3_5G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_5g_all_data.npz\",\n",
    "    \"DB3_10G\": r\"D:\\Stash\\Datasets\\db3_janelado\\db3_10g_all_data.npz\",\n",
    "    \"DB5_5G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_5g_all_data.npz\",\n",
    "    \"DB5_10G\": r\"D:\\Stash\\Datasets\\db5_janelado\\db5_10g_all_data.npz\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for db_name, data_path in datasets.items():\n",
    "    print(f\"\\nüîπ Iniciando otimiza√ß√£o BAYESIANA CNN+LSTM para {db_name}\")\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    mpp_train, mzp_train = calculate_mpp_mzp(X_train)\n",
    "    mpp_test, mzp_test = calculate_mpp_mzp(X_test)\n",
    "    X_train_final = np.concatenate([mpp_train, mzp_train], axis=-1)[..., np.newaxis]\n",
    "    X_test_final = np.concatenate([mpp_test, mzp_test], axis=-1)[..., np.newaxis]\n",
    "\n",
    "    def build_model(hp):\n",
    "        model = Sequential([\n",
    "            Input(shape=(X_train_final.shape[1], 1)),\n",
    "            Conv1D(filters=hp.Choice('conv1_filters', [16, 32, 64, 128]), kernel_size=hp.Choice('conv1_kernel', [2, 3, 5, 7]), padding='same', kernel_initializer='he_normal'),\n",
    "            BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "            Conv1D(filters=hp.Choice('conv2_filters', [32, 64, 128, 256]), kernel_size=hp.Choice('conv2_kernel', [2, 3, 5]), padding='same', kernel_initializer='he_normal'),\n",
    "            BatchNormalization(), Activation('relu'), MaxPooling1D(pool_size=2),\n",
    "            Conv1D(filters=hp.Choice('conv3_filters', [16, 32, 64, 128]), kernel_size=hp.Choice('conv3_kernel', [2, 3, 5]), padding='same', kernel_initializer='he_normal'),\n",
    "            BatchNormalization(), Activation('relu'),\n",
    "            Reshape((-1, hp.Choice('reshape_features', [16, 32, 64, 128]))),\n",
    "            LSTM(hp.Int('lstm_units', 32, 128, step=32), return_sequences=False),\n",
    "            Dense(hp.Int('dense1_units', 64, 512, step=64), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "            Dropout(hp.Float('dropout1', 0.3, 0.7, step=0.1)),\n",
    "            Dense(hp.Int('dense2_units', 32, 256, step=32), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "            Dropout(hp.Float('dropout2', 0.3, 0.7, step=0.1)),\n",
    "            Dense(len(np.unique(y_train)), activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=hp.Float('lr', 1e-5, 1e-3, sampling='log')), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    project_dir = os.path.join(\"CNN_LSTM_tuner_dir\", db_name)\n",
    "    tuner = kt.BayesianOptimization(build_model, objective='val_accuracy', max_trials=30, directory=project_dir, project_name='cnn_lstm_gesture_classification')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train_final, y_train, epochs=30, batch_size=128, validation_data=(X_test_final, y_test), callbacks=[early_stop], verbose=1)\n",
    "\n",
    "    best_model = tuner.get_best_models(1)[0]\n",
    "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "    y_pred = np.argmax(best_model.predict(X_test_final), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"‚úÖ Melhor acur√°cia H√çBRIDA (otimizada) para {db_name}: {accuracy:.4f}\")\n",
    "    print(f\"Melhores hiperpar√¢metros para {db_name}: {best_hp.values}\")\n",
    "\n",
    "    results.append({'Dataset': db_name, 'Accuracy': accuracy, **best_hp.values})\n",
    "\n",
    "    model_filename = f\"modelo_hibrido_otimizado_{db_name}.h5\"\n",
    "    best_model.save(model_filename)\n",
    "    print(f\"‚úÖ Modelo salvo como {model_filename}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(f'Curvas H√çBRIDAS OTIMIZADAS para {db_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"curva_hibrido_otimizado_{db_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"CNN_LSTM_hibrido_otimizado_todosDBs.csv\", index=False)\n",
    "print(\"‚úÖ Resultados H√çBRIDOS OTIMIZADOS salvos em CNN_LSTM_hibrido_otimizado_todosDBs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc5f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 02m 17s]\n",
      "val_accuracy: 0.5043230652809143\n",
      "\n",
      "Best val_accuracy So Far: 0.652572751045227\n",
      "Total elapsed time: 04h 10m 39s\n",
      "Melhores hiperpar√¢metros encontrados: {'conv1_filters': 96, 'conv1_kernel': 3, 'conv1_dropout': 0.4, 'conv_blocks_additional': 0, 'dense_units': 128, 'learning_rate': 0.0005, 'conv2_filters': 96, 'conv2_kernel': 5, 'pooling2': 'avg', 'conv2_dropout': 0.2, 'conv3_filters': 64, 'conv3_kernel': 5, 'pooling3': 'avg', 'conv3_dropout': 0.0, 'dense_dropout': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EMG_Gesture_CNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"EMG_Gesture_CNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,552</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_pooling1d        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m96\u001b[0m)        ‚îÇ         \u001b[38;5;34m3,552\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m96\u001b[0m)        ‚îÇ           \u001b[38;5;34m384\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m96\u001b[0m)        ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m96\u001b[0m)        ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_pooling1d        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m12,416\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              ‚îÇ           \u001b[38;5;34m645\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,997</span> (66.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,997\u001b[0m (66.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,805</span> (65.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,805\u001b[0m (65.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m297/297\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6861 - loss: 0.8994\n",
      "Acur√°cia no conjunto de teste: 65.26%\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68      1837\n",
      "           1       0.64      0.69      0.67      1857\n",
      "           2       0.64      0.56      0.60      1896\n",
      "           3       0.62      0.70      0.66      2009\n",
      "           4       0.70      0.61      0.66      1885\n",
      "\n",
      "    accuracy                           0.65      9484\n",
      "   macro avg       0.66      0.65      0.65      9484\n",
      "weighted avg       0.65      0.65      0.65      9484\n",
      "\n",
      "F1-score (macro): 0.6518356802698515\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras_tuner as kt\n",
    "\n",
    "data_path = r\"D:\\Stash\\Datasets\\db3_janelado\\db3_5g_all_data.npz\"\n",
    "data = np.load(data_path)\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "print(\"Formato final dos dados:\", X_train.shape)\n",
    "input_shape = (500, 12)   \n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential(name=\"EMG_Gesture_CNN\")\n",
    "    model.add(layers.Conv1D(\n",
    "        filters=hp.Int('conv1_filters', 32, 128, step=32),\n",
    "        kernel_size=hp.Choice('conv1_kernel', [3, 5, 7]), \n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Dropout(hp.Float('conv1_dropout', 0.0, 0.5, step=0.1)))\n",
    "\n",
    "    conv_blocks = hp.Int('conv_blocks_additional', 0, 2)\n",
    "    for i in range(conv_blocks):\n",
    "        filters = hp.Int(f'conv{i+2}_filters', 32, 128, step=32)\n",
    "        kernel_size = hp.Choice(f'conv{i+2}_kernel', [3, 5, 7])\n",
    "        model.add(layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                activation='relu', padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        pool_type = hp.Choice(f'pooling{i+2}', ['max', 'avg'])\n",
    "        if pool_type == 'max':\n",
    "            model.add(layers.MaxPooling1D(pool_size=2))\n",
    "        else:\n",
    "            model.add(layers.AveragePooling1D(pool_size=2))\n",
    "        model.add(layers.Dropout(hp.Float(f'conv{i+2}_dropout', 0.0, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    dense_units = hp.Int('dense_units', 0, 128, step=32)\n",
    "    if dense_units > 0:\n",
    "        model.add(layers.Dense(dense_units, activation='relu'))\n",
    "        model.add(layers.Dropout(hp.Float('dense_dropout', 0.0, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    directory='my_tuner_dir',\n",
    "    project_name='sEMG_gesture_CNN_tuning'\n",
    ")\n",
    "\n",
    "\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=30, \n",
    "             validation_data=(X_test, y_test),\n",
    "             batch_size=64,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])\n",
    "\n",
    "\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Melhores hiperpar√¢metros encontrados:\", best_hp.values)\n",
    "best_model.summary()\n",
    "\n",
    "\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Acur√°cia no conjunto de teste: {accuracy:.2%}\")\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = y_test  \n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "print(\"F1-score (macro):\", f1_score(y_true, y_pred_classes, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18911, 144)\n",
      "(18911,)\n",
      "(9484, 144)\n",
      "(9484,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# üîπ Configura√ß√£o do tuner\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mkt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBayesianOptimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtuner_dir_db3_5g_9f\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdnn_features_classification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     68\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# üîπ Executar o tuner\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras_tuner\\src\\tuners\\bayesian.py:394\u001b[0m, in \u001b[0;36mBayesianOptimization.__init__\u001b[1;34m(self, hypermodel, objective, max_trials, num_initial_points, alpha, beta, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    367\u001b[0m     hypermodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    380\u001b[0m ):\n\u001b[0;32m    381\u001b[0m     oracle \u001b[38;5;241m=\u001b[39m BayesianOptimizationOracle(\n\u001b[0;32m    382\u001b[0m         objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[0;32m    383\u001b[0m         max_trials\u001b[38;5;241m=\u001b[39mmax_trials,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m         max_consecutive_failed_trials\u001b[38;5;241m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[0;32m    393\u001b[0m     )\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:122\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[1;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hypermodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial \u001b[38;5;129;01mis\u001b[39;00m Tuner\u001b[38;5;241m.\u001b[39mrun_trial:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`hypermodel` if the user defines the search space in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing a `HyperModel` instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_model_size \u001b[38;5;241m=\u001b[39m max_model_size\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:132\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_populate_initial_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Run in distributed mode.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist_utils\u001b[38;5;241m.\u001b[39mhas_chief_oracle() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist_utils\u001b[38;5;241m.\u001b[39mis_chief_oracle():\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Proxies requests to the chief oracle.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Avoid import at the top, to avoid inconsistent protobuf versions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:192\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mdeclare_hyperparameters(hp)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activate_all_conditions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\TCC_2025\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:149\u001b[0m, in \u001b[0;36mBaseTuner._activate_all_conditions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_space()\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# Update the recorded scopes.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(hp)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_model\u001b[39m(hp):\n\u001b[0;32m     36\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m     37\u001b[0m         Input(shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[0;32m     38\u001b[0m         Dense(units\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense1_units\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m512\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m),\n\u001b[0;32m     39\u001b[0m               activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 40\u001b[0m               kernel_regularizer\u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(hp\u001b[38;5;241m.\u001b[39mFloat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m, sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m))),\n\u001b[0;32m     41\u001b[0m         BatchNormalization(),\n\u001b[0;32m     42\u001b[0m         Dropout(hp\u001b[38;5;241m.\u001b[39mFloat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)),\n\u001b[0;32m     43\u001b[0m         \n\u001b[0;32m     44\u001b[0m         Dense(units\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense2_units\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m256\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     45\u001b[0m               activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     46\u001b[0m               kernel_regularizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(hp\u001b[38;5;241m.\u001b[39mFloat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m, sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m))),\n\u001b[0;32m     47\u001b[0m         BatchNormalization(),\n\u001b[0;32m     48\u001b[0m         Dropout(hp\u001b[38;5;241m.\u001b[39mFloat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)),\n\u001b[0;32m     49\u001b[0m         \n\u001b[0;32m     50\u001b[0m         Dense(\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train)), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m     ])\n\u001b[0;32m     53\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     54\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mFloat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m, sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     55\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     56\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     57\u001b[0m     )\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "test_path = r\"D:\\Stash\\Datasets\\db3_features_all\\5g\\features_test.npz\"\n",
    "train_path =  r\"D:\\Stash\\Datasets\\db3_features_all\\5g\\features_train.npz\"\n",
    "\n",
    "teste = np.load(test_path)\n",
    "treino = np.load(train_path)\n",
    "X_train = treino[\"X\"]\n",
    "y_train = treino[\"y\"]\n",
    "X_test = teste[\"X\"]\n",
    "y_test = teste[\"y\"]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(units=hp.Int('dense1_units', 128, 512, step=128),\n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(hp.Float('dropout1', 0.3, 0.7, step=0.1)),\n",
    "        \n",
    "        Dense(units=hp.Int('dense2_units', 64, 256, step=64),\n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2', 1e-4, 1e-2, sampling='log'))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(hp.Float('dropout2', 0.3, 0.7, step=0.1)),\n",
    "        \n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('lr', 1e-4, 1e-3, sampling='log')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='tuner_dir_db3_5g_9f',\n",
    "    project_name='dnn_features_classification'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=5,\n",
    "             batch_size=256,\n",
    "             validation_data=(X_test, y_test),\n",
    "             callbacks=[early_stop],\n",
    "             verbose=2)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "\n",
    "output_dir = \"resultados_db3_5g\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(os.path.join(output_dir, \"best_hyperparameters.txt\"), \"w\") as f:\n",
    "    for param, value in best_hp.values.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "\n",
    "best_model.save(os.path.join(output_dir, \"best_model.h5\"))\n",
    "\n",
    "\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Melhor acur√°cia DNN otimizada (DB2 - 5g): {accuracy:.4f}\")\n",
    "with open(os.path.join(output_dir, \"final_accuracy.txt\"), \"w\") as f:\n",
    "    f.write(f\"Final accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_train))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "with open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
