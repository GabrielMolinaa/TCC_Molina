{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'emg', 'acc', 'stimulus', 'glove', 'subject', 'exercise', 'repetition', 'restimulus', 'rerepetition', 'age', 'circumference', 'frequency', 'gender', 'height', 'weight', 'laterality', 'sensor'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"S1_E1_A1.mat\"\n",
    "mat_data = scipy.io.loadmat(file_path)\n",
    "mat_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processamento paralelo...\n",
      "Iniciando processamento dos arquivos .mat...\n",
      "Arquivo encontrado: S10_E1_A1.mat\n",
      "Arquivo encontrado: S1_E1_A1.mat\n",
      "Arquivo encontrado: S2_E1_A1.mat\n",
      "Arquivo encontrado: S3_E1_A1.mat\n",
      "Arquivo encontrado: S4_E1_A1.mat\n",
      "Arquivo encontrado: S5_E1_A1.mat\n",
      "Arquivo encontrado: S6_E1_A1.mat\n",
      "Arquivo encontrado: S7_E1_A1.mat\n",
      "Arquivo encontrado: S8_E1_A1.mat\n",
      "Arquivo encontrado: S9_E1_A1.mat\n",
      "Processamento concluído para sujeito 5, exercício 1\n",
      "Processamento concluído para sujeito 1, exercício 1\n",
      "Processamento concluído para sujeito 3, exercício 1\n",
      "Processamento concluído para sujeito 9, exercício 1\n",
      "Processamento concluído para sujeito 8, exercício 1\n",
      "Processamento concluído para sujeito 7, exercício 1\n",
      "Processamento concluído para sujeito 4, exercício 1\n",
      "Processamento concluído para sujeito 10, exercício 1\n",
      "Processamento concluído para sujeito 6, exercício 1\n",
      "Processamento concluído para sujeito 2, exercício 1\n",
      "Processamento paralelo concluído e arquivos salvos.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nina_funcs import get_data, normalise, filter_data, windowing, get_categorical\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "# Definição dos diretórios\n",
    "base_dir = r\"C:\\Users\\molinaa\\Downloads\\DB5\"\n",
    "output_dir = r\"C:\\Users\\molinaa\\Downloads\\DB5_jan\"\n",
    "\n",
    "# Definição das repetições de treino e teste\n",
    "train_reps = [1, 3, 4, 6]\n",
    "test_reps = [2, 5]\n",
    "\n",
    "# Parâmetros da janela\n",
    "win_len = 40\n",
    "win_stride = 20\n",
    "\n",
    "# Mapeamento dos gestos por exercício\n",
    "gestures_dict = {\n",
    "    \"1\": [1, 2, 3, 4, 5, 6 ,7 ,8 , 9, 10],\n",
    "}\n",
    "\n",
    "def preprocess_and_save_data(file):\n",
    "    \"\"\"Processa e salva os dados de um arquivo .mat específico.\"\"\"\n",
    "    try:\n",
    "        # Extração dos IDs do sujeito e do exercício\n",
    "        parts = file.split('_')\n",
    "        subject_id = parts[0][1:]  # \"S1\" -> \"1\"\n",
    "        exercise_id = parts[1][1]  # \"E1\" -> \"1\"\n",
    "\n",
    "        # Verificar se o exercício é válido no dicionário\n",
    "        if exercise_id not in gestures_dict:\n",
    "            return\n",
    "\n",
    "        gestures = gestures_dict[exercise_id]\n",
    "\n",
    "        # Carregar os dados\n",
    "        data = get_data(base_dir, file)\n",
    "\n",
    "        # Normalização e filtragem\n",
    "        data = normalise(data, train_reps)\n",
    "        emg_band = filter_data(data=data, f=(2, 50), butterworth_order=4, btype='bandpass')\n",
    "\n",
    "        # Janelamento\n",
    "        X_train, y_train, _ = windowing(emg_band, train_reps, gestures, win_len, win_stride)\n",
    "        X_test, y_test, _ = windowing(emg_band, test_reps, gestures, win_len, win_stride)\n",
    "\n",
    "        # Codificação categórica\n",
    "        y_train = get_categorical(y_train)\n",
    "        y_test = get_categorical(y_test)\n",
    "        \n",
    "        # Converter one-hot encoding para rótulos inteiros, se necessário\n",
    "        if y_train.ndim > 1:\n",
    "            y_train = np.argmax(y_train, axis=1)\n",
    "        if y_test.ndim > 1:\n",
    "            y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "        # Salvar os arquivos processados\n",
    "        np.save(os.path.join(output_dir, f\"S{subject_id}E{exercise_id}_X_train.npy\"), X_train)\n",
    "        np.save(os.path.join(output_dir, f\"S{subject_id}E{exercise_id}_y_train.npy\"), y_train)\n",
    "        np.save(os.path.join(output_dir, f\"S{subject_id}E{exercise_id}_X_test.npy\"), X_test)\n",
    "        np.save(os.path.join(output_dir, f\"S{subject_id}E{exercise_id}_y_test.npy\"), y_test)\n",
    "        \n",
    "        print(f\"Processamento concluído para sujeito {subject_id}, exercício {exercise_id}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro no processamento do arquivo {file}: {e}\")\n",
    "\n",
    "def process_individual_files():\n",
    "    tasks = []\n",
    "    print(\"Iniciando processamento dos arquivos .mat...\")\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:  # Substituindo ProcessPoolExecutor\n",
    "        for file in os.listdir(base_dir):\n",
    "            if file.endswith(\".mat\"):\n",
    "                print(f\"Arquivo encontrado: {file}\")\n",
    "                tasks.append(executor.submit(preprocess_and_save_data, file))\n",
    "\n",
    "        # Aguardar a conclusão de todas as tarefas\n",
    "        for future in as_completed(tasks):\n",
    "            future.result()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Iniciando processamento paralelo...\")\n",
    "    process_individual_files()\n",
    "    print(\"Processamento paralelo concluído e arquivos salvos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extração de features para arquivos janelados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\molinaa\\Desktop\\Projeto_Estagio\\clean_venv\\Lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extraídas e salvas para S1E1\n",
      "Features extraídas e salvas para S2E1\n",
      "Features extraídas e salvas para S3E1\n",
      "Features extraídas e salvas para S4E1\n",
      "Features extraídas e salvas para S5E1\n",
      "Features extraídas e salvas para S6E1\n",
      "Features extraídas e salvas para S7E1\n",
      "Features extraídas e salvas para S8E1\n",
      "Features extraídas e salvas para S9E1\n",
      "Features extraídas e salvas para S10E1\n",
      "Extração de features concluída.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "# Diretório onde os arquivos janelados foram salvos\n",
    "input_dir = r\"C:\\Users\\molinaa\\Downloads\\DB5_jan\"\n",
    "output_dir = r\"C:\\Users\\molinaa\\Downloads\\DB5_feats\"\n",
    "\n",
    "# Função para extração de features de uma única janela de EMG\n",
    "def extract_features(emg_window, sampling_rate=200):\n",
    "    features = []\n",
    "    for channel in range(emg_window.shape[1]):\n",
    "        signal_data = emg_window[:, channel]\n",
    "\n",
    "        # Domínio Temporal\n",
    "        mean_abs = np.mean(np.abs(signal_data))\n",
    "        rms = np.sqrt(np.mean(signal_data ** 2))\n",
    "        variance = np.var(signal_data)\n",
    "        wl = np.sum(np.abs(np.diff(signal_data)))  # Waveform Length\n",
    "        #ssc = np.sum(np.diff(signal_data, 2) > 0)  # Slope Sign Change\n",
    "        zc = np.sum(np.diff(np.sign(signal_data)) != 0)  # Zero Crossings\n",
    "\n",
    "        # Domínio Espectral\n",
    "        f, psd = signal.welch(signal_data, fs=sampling_rate, nperseg=len(signal_data))\n",
    "        spectral_entropy = -np.sum(psd * np.log(psd + 1e-12)) \n",
    "        mean_freq = np.sum(f * psd) / np.sum(psd)  # Frequência Média\n",
    "\n",
    "        coeffs = pywt.wavedec(signal_data, 'db7', level=3)\n",
    "        mdwt_energy = [np.sum(np.square(c)) for c in coeffs]\n",
    "        \n",
    "        # Adiciona todas as features à lista\n",
    "        features.extend([rms,spectral_entropy] + mdwt_energy)\n",
    "    return features\n",
    "\n",
    "# Função para aplicar extração de features em todas as janelas de um dataset\n",
    "def extract_features_from_dataset(X_windows):\n",
    "    return np.array([extract_features(window) for window in X_windows])\n",
    "\n",
    "# Processar arquivos janelados e salvar features\n",
    "def process_extracted_windows():\n",
    "    for subject_id in range(1, 11):  # Indo do S1 ao S10\n",
    "        for exercise_id in range(1, 2):  # Assumindo que há apenas E1 para cada sujeito\n",
    "            try:\n",
    "                # Carregar os arquivos janelados\n",
    "                X_train = np.load(os.path.join(input_dir, f\"S{subject_id}E{exercise_id}_X_train.npy\"))\n",
    "                X_test = np.load(os.path.join(input_dir, f\"S{subject_id}E{exercise_id}_X_test.npy\"))\n",
    "\n",
    "                # Extrair features\n",
    "                X_train_feats = extract_features_from_dataset(X_train)\n",
    "                X_test_feats = extract_features_from_dataset(X_test)\n",
    "\n",
    "                \n",
    "                np.save(os.path.join(output_dir, f\"S{subject_id}E{exercise_id}_X_train_feats.npy\"), X_train_feats)\n",
    "                np.save(os.path.join(output_dir, f\"S{subject_id}E{exercise_id}_X_test_feats.npy\"), X_test_feats)\n",
    "\n",
    "                print(f\"Features extraídas e salvas para S{subject_id}E{exercise_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar S{subject_id}E{exercise_id}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Iniciando extração de features para arquivos janelados...\")\n",
    "    process_extracted_windows()\n",
    "    print(\"Extração de features concluída.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sujeito: 1\n",
      "Sujeito: 2\n",
      "Sujeito: 3\n",
      "Sujeito: 4\n",
      "Sujeito: 5\n",
      "Sujeito: 6\n",
      "Sujeito: 7\n",
      "Sujeito: 8\n",
      "Sujeito: 9\n",
      "Sujeito: 10\n",
      "Sujeito: 11\n",
      "Sujeito: 12\n",
      "Sujeito: 13\n",
      "Sujeito: 14\n",
      "Sujeito: 15\n",
      "Sujeito: 16\n",
      "Sujeito: 17\n",
      "Sujeito: 18\n",
      "Sujeito: 19\n",
      "Sujeito: 20\n",
      "Sujeito: 21\n",
      "Sujeito: 22\n",
      "Sujeito: 23\n",
      "Sujeito: 24\n",
      "Sujeito: 25\n",
      "Sujeito: 26\n",
      "Sujeito: 27\n",
      "Sujeito: 28\n",
      "Sujeito: 29\n",
      "Sujeito: 30\n",
      "Sujeito: 31\n",
      "Sujeito: 32\n",
      "Sujeito: 33\n",
      "Sujeito: 34\n",
      "Sujeito: 35\n",
      "Sujeito: 36\n",
      "Sujeito: 37\n",
      "Sujeito: 38\n",
      "Sujeito: 39\n",
      "Sujeito: 40\n",
      "Treinando: Random Forest\n",
      "Prevendo: Random Forest\n",
      "Treinando: SVM\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Diretório onde os arquivos de features foram salvos\n",
    "features_train_path = r\"C:\\Users\\PC\\Desktop\\TCC_2025\\App\\Data\\db2_features\\features_train.npz\"\n",
    "features_test_path = r\"C:\\Users\\PC\\Desktop\\TCC_2025\\App\\Data\\db2_features\\features_test.npz\"\n",
    "\n",
    "\n",
    "# Opção de treinamento: \"individual\" para treinar um modelo por sujeito, \"global\" para treinar com todos os sujeitos\n",
    "train_mode = \"global\"  # Alterar para \"global\" se desejar treinar um único modelo\n",
    "\n",
    "# Função para carregar os dados por sujeito\n",
    "def load_feature_data(subject_id):\n",
    "    try:\n",
    "        train_data = np.load(features_train_path)\n",
    "        test_data = np.load(features_test_path)\n",
    "\n",
    "        X_train, y_train = train_data[\"X\"], train_data[\"y\"]\n",
    "        X_test, y_test = test_data[\"X\"], test_data[\"y\"]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados de S{subject_id}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=10),\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"XGBoost\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "subject_accuracies = []\n",
    "\n",
    "if train_mode == \"individual\":\n",
    "    for subject_id in range(1, 11):\n",
    "        X_train, X_test, y_train, y_test = load_feature_data(subject_id)\n",
    "        if X_train is None:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            subject_accuracies.append({\"Sujeito\": f\"S{subject_id}\", \"Modelo\": name, \"Acurácia\": acc})\n",
    "\n",
    "elif train_mode == \"global\":\n",
    "    X_train_all, X_test_all, y_train_all, y_test_all = [], [], [], []\n",
    "    for subject_id in range(1, 41):\n",
    "        print(\"Sujeito:\", subject_id)\n",
    "        X_train, X_test, y_train, y_test = load_feature_data(subject_id)\n",
    "        if X_train is None:\n",
    "            continue\n",
    "        X_train_all.append(X_train)\n",
    "        X_test_all.append(X_test)\n",
    "        y_train_all.append(y_train)\n",
    "        y_test_all.append(y_test)\n",
    "    \n",
    "    X_train_all = np.vstack(X_train_all)\n",
    "    X_test_all = np.vstack(X_test_all)\n",
    "    y_train_all = np.hstack(y_train_all)\n",
    "    y_test_all = np.hstack(y_test_all)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_all)\n",
    "    X_test_scaled = scaler.transform(X_test_all)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(\"Treinando:\", name)\n",
    "        model.fit(X_train_scaled, y_train_all)\n",
    "        \n",
    "        print(\"Prevendo:\", name)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        acc = accuracy_score(y_test_all, y_pred)\n",
    "        subject_accuracies.append({\"Sujeito\": \"Global\", \"Modelo\": name, \"Acurácia\": acc})\n",
    "\n",
    "# Converter resultados para DataFrame e visualizar\n",
    "import pandas as pd\n",
    "subject_accuracies_df = pd.DataFrame(subject_accuracies)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Sujeito\", y=\"Acurácia\", hue=\"Modelo\", data=subject_accuracies_df)\n",
    "plt.title(\"Acurácia por Sujeito e Modelo\")\n",
    "plt.xlabel(\"Sujeito\")\n",
    "plt.ylabel(\"Acurácia\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Modelo\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sujeito",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Modelo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Acurácia",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3154b250-7840-4f6d-8cc0-2f5c6818fb35",
       "rows": [
        [
         "0",
         "Global",
         "Random Forest",
         "0.8571588429474627"
        ],
        [
         "1",
         "Global",
         "SVM",
         "0.8211548145247021"
        ],
        [
         "2",
         "Global",
         "KNN",
         "0.76959659822078"
        ],
        [
         "3",
         "Global",
         "LR",
         "0.5079729200469982"
        ],
        [
         "4",
         "Global",
         "XGBoost",
         "0.6671235942483075"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sujeito</th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Acurácia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.857159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.821155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.769597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.507973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.667124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sujeito         Modelo  Acurácia\n",
       "0  Global  Random Forest  0.857159\n",
       "1  Global            SVM  0.821155\n",
       "2  Global            KNN  0.769597\n",
       "3  Global             LR  0.507973\n",
       "4  Global        XGBoost  0.667124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_accuracies_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
